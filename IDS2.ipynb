{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Modelling Intrusion Detection: Analysis of a Feature Selection Mechanism\n",
    "Method Description\n",
    "Step 1: Data preprocessing:\n",
    "All features are made numerical using one-Hot-encoding. The features are scaled to avoid features with large values that may weigh too much in the results.\n",
    "\n",
    "Step 2: Feature Selection:\n",
    "Eliminate redundant and irrelevant data by selecting a subset of relevant features that fully represents the given problem. Univariate feature selection with ANOVA F-test. This analyzes each feature individually to detemine the strength of the relationship between the feature and labels. Using SecondPercentile method (sklearn.feature_selection) to select features based on percentile of the highest scores. When this subset is found: Recursive Feature Elimination (RFE) is applied.\n",
    "\n",
    "Step 4: Build the model:\n",
    "Decision tree model is built.\n",
    "\n",
    "Step 5: Prediction & Evaluation (validation):\n",
    "Using the test data to make predictions of the model. Multiple scores are considered such as:accuracy score, recall, f-measure, confusion matrix. perform a 10-fold cross-validation.\n",
    "\n",
    "Version Check\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(sys.version)\n",
    "print(sklearn.__version__)\n",
    "0.19.2\n",
    "1.11.3\n",
    "3.6.0 |Anaconda 4.3.0 (64-bit)| (default, Dec 23 2016, 11:57:41) [MSC v.1900 64 bit (AMD64)]\n",
    "0.18.1\n",
    "Load the Dataset\n",
    "# attach the column names to the dataset\n",
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
    "\n",
    "# KDDTrain+_2.csv & KDDTest+_2.csv are the datafiles without the last column about the difficulty score\n",
    "# these have already been removed.\n",
    "df = pd.read_csv(\"KDDTrain+_2.csv\", header=None, names = col_names)\n",
    "df_test = pd.read_csv(\"KDDTest+_2.csv\", header=None, names = col_names)\n",
    "\n",
    "# shape, this gives the dimensions of the dataset\n",
    "print('Dimensions of the Training set:',df.shape)\n",
    "print('Dimensions of the Test set:',df_test.shape)\n",
    "Dimensions of the Training set: (125973, 42)\n",
    "Dimensions of the Test set: (22544, 42)\n",
    "Sample view of the training dataset\n",
    "# first five rows\n",
    "df.head(5)\n",
    "duration\tprotocol_type\tservice\tflag\tsrc_bytes\tdst_bytes\tland\twrong_fragment\turgent\thot\t...\tdst_host_srv_count\tdst_host_same_srv_rate\tdst_host_diff_srv_rate\tdst_host_same_src_port_rate\tdst_host_srv_diff_host_rate\tdst_host_serror_rate\tdst_host_srv_serror_rate\tdst_host_rerror_rate\tdst_host_srv_rerror_rate\tlabel\n",
    "0\t0\ttcp\tftp_data\tSF\t491\t0\t0\t0\t0\t0\t...\t25\t0.17\t0.03\t0.17\t0.00\t0.00\t0.00\t0.05\t0.00\tnormal\n",
    "1\t0\tudp\tother\tSF\t146\t0\t0\t0\t0\t0\t...\t1\t0.00\t0.60\t0.88\t0.00\t0.00\t0.00\t0.00\t0.00\tnormal\n",
    "2\t0\ttcp\tprivate\tS0\t0\t0\t0\t0\t0\t0\t...\t26\t0.10\t0.05\t0.00\t0.00\t1.00\t1.00\t0.00\t0.00\tneptune\n",
    "3\t0\ttcp\thttp\tSF\t232\t8153\t0\t0\t0\t0\t...\t255\t1.00\t0.00\t0.03\t0.04\t0.03\t0.01\t0.00\t0.01\tnormal\n",
    "4\t0\ttcp\thttp\tSF\t199\t420\t0\t0\t0\t0\t...\t255\t1.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\tnormal\n",
    "5 rows × 42 columns\n",
    "\n",
    "Statistical Summary\n",
    "df.describe()\n",
    "duration\tsrc_bytes\tdst_bytes\tland\twrong_fragment\turgent\thot\tnum_failed_logins\tlogged_in\tnum_compromised\t...\tdst_host_count\tdst_host_srv_count\tdst_host_same_srv_rate\tdst_host_diff_srv_rate\tdst_host_same_src_port_rate\tdst_host_srv_diff_host_rate\tdst_host_serror_rate\tdst_host_srv_serror_rate\tdst_host_rerror_rate\tdst_host_srv_rerror_rate\n",
    "count\t125973.00000\t1.259730e+05\t1.259730e+05\t125973.000000\t125973.000000\t125973.000000\t125973.000000\t125973.000000\t125973.000000\t125973.000000\t...\t125973.000000\t125973.000000\t125973.000000\t125973.000000\t125973.000000\t125973.000000\t125973.000000\t125973.000000\t125973.000000\t125973.000000\n",
    "mean\t287.14465\t4.556674e+04\t1.977911e+04\t0.000198\t0.022687\t0.000111\t0.204409\t0.001222\t0.395736\t0.279250\t...\t182.148945\t115.653005\t0.521242\t0.082951\t0.148379\t0.032542\t0.284452\t0.278485\t0.118832\t0.120240\n",
    "std\t2604.51531\t5.870331e+06\t4.021269e+06\t0.014086\t0.253530\t0.014366\t2.149968\t0.045239\t0.489010\t23.942042\t...\t99.206213\t110.702741\t0.448949\t0.188922\t0.308997\t0.112564\t0.444784\t0.445669\t0.306557\t0.319459\n",
    "min\t0.00000\t0.000000e+00\t0.000000e+00\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t...\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\n",
    "25%\t0.00000\t0.000000e+00\t0.000000e+00\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t...\t82.000000\t10.000000\t0.050000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\n",
    "50%\t0.00000\t4.400000e+01\t0.000000e+00\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t...\t255.000000\t63.000000\t0.510000\t0.020000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\n",
    "75%\t0.00000\t2.760000e+02\t5.160000e+02\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t1.000000\t0.000000\t...\t255.000000\t255.000000\t1.000000\t0.070000\t0.060000\t0.020000\t1.000000\t1.000000\t0.000000\t0.000000\n",
    "max\t42908.00000\t1.379964e+09\t1.309937e+09\t1.000000\t3.000000\t3.000000\t77.000000\t5.000000\t1.000000\t7479.000000\t...\t255.000000\t255.000000\t1.000000\t1.000000\t1.000000\t1.000000\t1.000000\t1.000000\t1.000000\t1.000000\n",
    "8 rows × 38 columns\n",
    "\n",
    "Label Distribution of Training and Test set\n",
    "print('Label distribution Training set:')\n",
    "print(df['label'].value_counts())\n",
    "print()\n",
    "print('Label distribution Test set:')\n",
    "print(df_test['label'].value_counts())\n",
    "Label distribution Training set:\n",
    "normal             67343\n",
    "neptune            41214\n",
    "satan               3633\n",
    "ipsweep             3599\n",
    "portsweep           2931\n",
    "smurf               2646\n",
    "nmap                1493\n",
    "back                 956\n",
    "teardrop             892\n",
    "warezclient          890\n",
    "pod                  201\n",
    "guess_passwd          53\n",
    "buffer_overflow       30\n",
    "warezmaster           20\n",
    "land                  18\n",
    "imap                  11\n",
    "rootkit               10\n",
    "loadmodule             9\n",
    "ftp_write              8\n",
    "multihop               7\n",
    "phf                    4\n",
    "perl                   3\n",
    "spy                    2\n",
    "Name: label, dtype: int64\n",
    "\n",
    "Label distribution Test set:\n",
    "normal             9711\n",
    "neptune            4657\n",
    "guess_passwd       1231\n",
    "mscan               996\n",
    "warezmaster         944\n",
    "apache2             737\n",
    "satan               735\n",
    "processtable        685\n",
    "smurf               665\n",
    "back                359\n",
    "snmpguess           331\n",
    "saint               319\n",
    "mailbomb            293\n",
    "snmpgetattack       178\n",
    "portsweep           157\n",
    "ipsweep             141\n",
    "httptunnel          133\n",
    "nmap                 73\n",
    "pod                  41\n",
    "buffer_overflow      20\n",
    "multihop             18\n",
    "named                17\n",
    "ps                   15\n",
    "sendmail             14\n",
    "rootkit              13\n",
    "xterm                13\n",
    "teardrop             12\n",
    "xlock                 9\n",
    "land                  7\n",
    "xsnoop                4\n",
    "ftp_write             3\n",
    "udpstorm              2\n",
    "perl                  2\n",
    "worm                  2\n",
    "loadmodule            2\n",
    "sqlattack             2\n",
    "phf                   2\n",
    "imap                  1\n",
    "Name: label, dtype: int64\n",
    "Step 1: Data preprocessing:\n",
    "One-Hot-Encoding (one-of-K) is used to to transform all categorical features into binary features. Requirement for One-Hot-encoding: \"The input to this transformer should be a matrix of integers, denoting the values taken on by categorical (discrete) features. The output will be a sparse matrix where each column corresponds to one possible value of one feature. It is assumed that input features take on values in the range [0, n_values).\"\n",
    "\n",
    "Therefore the features first need to be transformed with LabelEncoder, to transform every category to a number.\n",
    "\n",
    "Identify categorical features\n",
    "# colums that are categorical and not binary yet: protocol_type (column 2), service (column 3), flag (column 4).\n",
    "# explore categorical features\n",
    "print('Training set:')\n",
    "for col_name in df.columns:\n",
    "    if df[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "#see how distributed the feature service is, it is evenly distributed and therefore we need to make dummies for all.\n",
    "print()\n",
    "print('Distribution of categories in service:')\n",
    "print(df['service'].value_counts().sort_values(ascending=False).head())\n",
    "Training set:\n",
    "Feature 'protocol_type' has 3 categories\n",
    "Feature 'service' has 70 categories\n",
    "Feature 'flag' has 11 categories\n",
    "Feature 'label' has 23 categories\n",
    "\n",
    "Distribution of categories in service:\n",
    "http        40338\n",
    "private     21853\n",
    "domain_u     9043\n",
    "smtp         7313\n",
    "ftp_data     6860\n",
    "Name: service, dtype: int64\n",
    "# Test set\n",
    "print('Test set:')\n",
    "for col_name in df_test.columns:\n",
    "    if df_test[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df_test[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "Test set:\n",
    "Feature 'protocol_type' has 3 categories\n",
    "Feature 'service' has 64 categories\n",
    "Feature 'flag' has 11 categories\n",
    "Feature 'label' has 38 categories\n",
    "Conclusion: Need to make dummies for all categories as the distribution is fairly even. In total: 3+70+11=84 dummies.\n",
    "Comparing the results shows that the Test set has fewer categories (6), these need to be added as empty columns.\n",
    "LabelEncoder\n",
    "Insert categorical features into a 2D numpy array\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "categorical_columns=['protocol_type', 'service', 'flag']\n",
    "# insert code to get a list of categorical columns into a variable, categorical_columns\n",
    "categorical_columns=['protocol_type', 'service', 'flag'] \n",
    " # Get the categorical values into a 2D numpy array\n",
    "df_categorical_values = df[categorical_columns]\n",
    "testdf_categorical_values = df_test[categorical_columns]\n",
    "df_categorical_values.head()\n",
    "protocol_type\tservice\tflag\n",
    "0\ttcp\tftp_data\tSF\n",
    "1\tudp\tother\tSF\n",
    "2\ttcp\tprivate\tS0\n",
    "3\ttcp\thttp\tSF\n",
    "4\ttcp\thttp\tSF\n",
    "Make column names for dummies\n",
    "# protocol type\n",
    "unique_protocol=sorted(df.protocol_type.unique())\n",
    "string1 = 'Protocol_type_'\n",
    "unique_protocol2=[string1 + x for x in unique_protocol]\n",
    "# service\n",
    "unique_service=sorted(df.service.unique())\n",
    "string2 = 'service_'\n",
    "unique_service2=[string2 + x for x in unique_service]\n",
    "# flag\n",
    "unique_flag=sorted(df.flag.unique())\n",
    "string3 = 'flag_'\n",
    "unique_flag2=[string3 + x for x in unique_flag]\n",
    "# put together\n",
    "dumcols=unique_protocol2 + unique_service2 + unique_flag2\n",
    "print(dumcols)\n",
    "\n",
    "#do same for test set\n",
    "unique_service_test=sorted(df_test.service.unique())\n",
    "unique_service2_test=[string2 + x for x in unique_service_test]\n",
    "testdumcols=unique_protocol2 + unique_service2_test + unique_flag2\n",
    "['Protocol_type_icmp', 'Protocol_type_tcp', 'Protocol_type_udp', 'service_IRC', 'service_X11', 'service_Z39_50', 'service_aol', 'service_auth', 'service_bgp', 'service_courier', 'service_csnet_ns', 'service_ctf', 'service_daytime', 'service_discard', 'service_domain', 'service_domain_u', 'service_echo', 'service_eco_i', 'service_ecr_i', 'service_efs', 'service_exec', 'service_finger', 'service_ftp', 'service_ftp_data', 'service_gopher', 'service_harvest', 'service_hostnames', 'service_http', 'service_http_2784', 'service_http_443', 'service_http_8001', 'service_imap4', 'service_iso_tsap', 'service_klogin', 'service_kshell', 'service_ldap', 'service_link', 'service_login', 'service_mtp', 'service_name', 'service_netbios_dgm', 'service_netbios_ns', 'service_netbios_ssn', 'service_netstat', 'service_nnsp', 'service_nntp', 'service_ntp_u', 'service_other', 'service_pm_dump', 'service_pop_2', 'service_pop_3', 'service_printer', 'service_private', 'service_red_i', 'service_remote_job', 'service_rje', 'service_shell', 'service_smtp', 'service_sql_net', 'service_ssh', 'service_sunrpc', 'service_supdup', 'service_systat', 'service_telnet', 'service_tftp_u', 'service_tim_i', 'service_time', 'service_urh_i', 'service_urp_i', 'service_uucp', 'service_uucp_path', 'service_vmnet', 'service_whois', 'flag_OTH', 'flag_REJ', 'flag_RSTO', 'flag_RSTOS0', 'flag_RSTR', 'flag_S0', 'flag_S1', 'flag_S2', 'flag_S3', 'flag_SF', 'flag_SH']\n",
    "Transform categorical features into numbers using LabelEncoder()\n",
    "df_categorical_values_enc=df_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "print(df_categorical_values_enc.head())\n",
    "# test set\n",
    "testdf_categorical_values_enc=testdf_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "   protocol_type  service  flag\n",
    "0              1       20     9\n",
    "1              2       44     9\n",
    "2              1       49     5\n",
    "3              1       24     9\n",
    "4              1       24     9\n",
    "One-Hot-Encoding\n",
    "enc = OneHotEncoder()\n",
    "df_categorical_values_encenc = enc.fit_transform(df_categorical_values_enc)\n",
    "df_cat_data = pd.DataFrame(df_categorical_values_encenc.toarray(),columns=dumcols)\n",
    "# test set\n",
    "testdf_categorical_values_encenc = enc.fit_transform(testdf_categorical_values_enc)\n",
    "testdf_cat_data = pd.DataFrame(testdf_categorical_values_encenc.toarray(),columns=testdumcols)\n",
    "\n",
    "df_cat_data.head()\n",
    "Protocol_type_icmp\tProtocol_type_tcp\tProtocol_type_udp\tservice_IRC\tservice_X11\tservice_Z39_50\tservice_aol\tservice_auth\tservice_bgp\tservice_courier\t...\tflag_REJ\tflag_RSTO\tflag_RSTOS0\tflag_RSTR\tflag_S0\tflag_S1\tflag_S2\tflag_S3\tflag_SF\tflag_SH\n",
    "0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\n",
    "1\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\n",
    "2\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "3\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\n",
    "4\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\n",
    "5 rows × 84 columns\n",
    "\n",
    "Add 6 missing categories from train set to test set\n",
    "trainservice=df['service'].tolist()\n",
    "testservice= df_test['service'].tolist()\n",
    "difference=list(set(trainservice) - set(testservice))\n",
    "string = 'service_'\n",
    "difference=[string + x for x in difference]\n",
    "difference\n",
    "['service_red_i',\n",
    " 'service_harvest',\n",
    " 'service_http_2784',\n",
    " 'service_http_8001',\n",
    " 'service_urh_i',\n",
    " 'service_aol']\n",
    "for col in difference:\n",
    "    testdf_cat_data[col] = 0\n",
    "\n",
    "testdf_cat_data.shape\n",
    "(22544, 84)\n",
    "Join encoded categorical dataframe with the non-categorical dataframe\n",
    "newdf=df.join(df_cat_data)\n",
    "newdf.drop('flag', axis=1, inplace=True)\n",
    "newdf.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf.drop('service', axis=1, inplace=True)\n",
    "# test data\n",
    "newdf_test=df_test.join(testdf_cat_data)\n",
    "newdf_test.drop('flag', axis=1, inplace=True)\n",
    "newdf_test.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf_test.drop('service', axis=1, inplace=True)\n",
    "print(newdf.shape)\n",
    "print(newdf_test.shape)\n",
    "(125973, 123)\n",
    "(22544, 123)\n",
    "Split Dataset into 4 datasets for every attack category\n",
    "Rename every attack label: 0=normal, 1=DoS, 2=Probe, 3=R2L and 4=U2R.\n",
    "Replace labels column with new labels column\n",
    "Make new datasets\n",
    "# take label column\n",
    "labeldf=newdf['label']\n",
    "labeldf_test=newdf_test['label']\n",
    "# change the label column\n",
    "newlabeldf=labeldf.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "newlabeldf_test=labeldf_test.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "# put the new label column back\n",
    "newdf['label'] = newlabeldf\n",
    "newdf_test['label'] = newlabeldf_test\n",
    "print(newdf['label'].head())\n",
    "0    0\n",
    "1    0\n",
    "2    1\n",
    "3    0\n",
    "4    0\n",
    "Name: label, dtype: int64\n",
    "to_drop_DoS = [2,3,4]\n",
    "to_drop_Probe = [1,3,4]\n",
    "to_drop_R2L = [1,2,4]\n",
    "to_drop_U2R = [1,2,3]\n",
    "DoS_df=newdf[~newdf['label'].isin(to_drop_DoS)];\n",
    "Probe_df=newdf[~newdf['label'].isin(to_drop_Probe)];\n",
    "R2L_df=newdf[~newdf['label'].isin(to_drop_R2L)];\n",
    "U2R_df=newdf[~newdf['label'].isin(to_drop_U2R)];\n",
    "\n",
    "#test\n",
    "DoS_df_test=newdf_test[~newdf_test['label'].isin(to_drop_DoS)];\n",
    "Probe_df_test=newdf_test[~newdf_test['label'].isin(to_drop_Probe)];\n",
    "R2L_df_test=newdf_test[~newdf_test['label'].isin(to_drop_R2L)];\n",
    "U2R_df_test=newdf_test[~newdf_test['label'].isin(to_drop_U2R)];\n",
    "print('Train:')\n",
    "print('Dimensions of DoS:' ,DoS_df.shape)\n",
    "print('Dimensions of Probe:' ,Probe_df.shape)\n",
    "print('Dimensions of R2L:' ,R2L_df.shape)\n",
    "print('Dimensions of U2R:' ,U2R_df.shape)\n",
    "print('Test:')\n",
    "print('Dimensions of DoS:' ,DoS_df_test.shape)\n",
    "print('Dimensions of Probe:' ,Probe_df_test.shape)\n",
    "print('Dimensions of R2L:' ,R2L_df_test.shape)\n",
    "print('Dimensions of U2R:' ,U2R_df_test.shape)\n",
    "Train:\n",
    "Dimensions of DoS: (113270, 123)\n",
    "Dimensions of Probe: (78999, 123)\n",
    "Dimensions of R2L: (68338, 123)\n",
    "Dimensions of U2R: (67395, 123)\n",
    "Test:\n",
    "Dimensions of DoS: (17171, 123)\n",
    "Dimensions of Probe: (12132, 123)\n",
    "Dimensions of R2L: (12596, 123)\n",
    "Dimensions of U2R: (9778, 123)\n",
    "Step 2: Feature Scaling:\n",
    "# Split dataframes into X & Y\n",
    "# assign X as a dataframe of feautures and Y as a series of outcome variables\n",
    "X_DoS = DoS_df.drop('label',1)\n",
    "Y_DoS = DoS_df.label\n",
    "X_Probe = Probe_df.drop('label',1)\n",
    "Y_Probe = Probe_df.label\n",
    "X_R2L = R2L_df.drop('label',1)\n",
    "Y_R2L = R2L_df.label\n",
    "X_U2R = U2R_df.drop('label',1)\n",
    "Y_U2R = U2R_df.label\n",
    "# test set\n",
    "X_DoS_test = DoS_df_test.drop('label',1)\n",
    "Y_DoS_test = DoS_df_test.label\n",
    "X_Probe_test = Probe_df_test.drop('label',1)\n",
    "Y_Probe_test = Probe_df_test.label\n",
    "X_R2L_test = R2L_df_test.drop('label',1)\n",
    "Y_R2L_test = R2L_df_test.label\n",
    "X_U2R_test = U2R_df_test.drop('label',1)\n",
    "Y_U2R_test = U2R_df_test.label\n",
    "Save a list of feature names for later use (it is the same for every attack category). Column names are dropped at this stage.\n",
    "colNames=list(X_DoS)\n",
    "colNames_test=list(X_DoS_test)\n",
    "Use StandardScaler() to scale the dataframes\n",
    "from sklearn import preprocessing\n",
    "scaler1 = preprocessing.StandardScaler().fit(X_DoS)\n",
    "X_DoS=scaler1.transform(X_DoS) \n",
    "scaler2 = preprocessing.StandardScaler().fit(X_Probe)\n",
    "X_Probe=scaler2.transform(X_Probe) \n",
    "scaler3 = preprocessing.StandardScaler().fit(X_R2L)\n",
    "X_R2L=scaler3.transform(X_R2L) \n",
    "scaler4 = preprocessing.StandardScaler().fit(X_U2R)\n",
    "X_U2R=scaler4.transform(X_U2R) \n",
    "# test data\n",
    "scaler5 = preprocessing.StandardScaler().fit(X_DoS_test)\n",
    "X_DoS_test=scaler5.transform(X_DoS_test) \n",
    "scaler6 = preprocessing.StandardScaler().fit(X_Probe_test)\n",
    "X_Probe_test=scaler6.transform(X_Probe_test) \n",
    "scaler7 = preprocessing.StandardScaler().fit(X_R2L_test)\n",
    "X_R2L_test=scaler7.transform(X_R2L_test) \n",
    "scaler8 = preprocessing.StandardScaler().fit(X_U2R_test)\n",
    "X_U2R_test=scaler8.transform(X_U2R_test) \n",
    "Check that the Standard Deviation is 1\n",
    "print(X_DoS.std(axis=0))\n",
    "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.\n",
    "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
    "  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
    "  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0.  1.  0.  1.  1.  1.\n",
    "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.\n",
    "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
    "  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.]\n",
    "X_Probe.std(axis=0);\n",
    "X_R2L.std(axis=0);\n",
    "X_U2R.std(axis=0);\n",
    "Step 3: Feature Selection:\n",
    "1. Univariate Feature Selection using ANOVA F-test\n",
    "#univariate feature selection with ANOVA F-test. using secondPercentile method, then RFE\n",
    "#Scikit-learn exposes feature selection routines as objects that implement the transform method\n",
    "#SelectPercentile: removes all but a user-specified highest scoring percentage of features\n",
    "#f_classif: ANOVA F-value between label/feature for classification tasks.\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "np.seterr(divide='ignore', invalid='ignore');\n",
    "selector=SelectPercentile(f_classif, percentile=10)\n",
    "X_newDoS = selector.fit_transform(X_DoS,Y_DoS)\n",
    "X_newDoS.shape\n",
    "C:\\Users\\Cynthia\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [ 16  44  63  66  68  86 114] are constant.\n",
    "  UserWarning)\n",
    "(113270, 13)\n",
    "Get the features that were selected: DoS\n",
    "true=selector.get_support()\n",
    "newcolindex_DoS=[i for i, x in enumerate(true) if x]\n",
    "newcolname_DoS=list( colNames[i] for i in newcolindex_DoS )\n",
    "newcolname_DoS\n",
    "['logged_in',\n",
    " 'count',\n",
    " 'serror_rate',\n",
    " 'srv_serror_rate',\n",
    " 'same_srv_rate',\n",
    " 'dst_host_count',\n",
    " 'dst_host_srv_count',\n",
    " 'dst_host_same_srv_rate',\n",
    " 'dst_host_serror_rate',\n",
    " 'dst_host_srv_serror_rate',\n",
    " 'service_http',\n",
    " 'flag_S0',\n",
    " 'flag_SF']\n",
    "X_newProbe = selector.fit_transform(X_Probe,Y_Probe)\n",
    "X_newProbe.shape\n",
    "C:\\Users\\Cynthia\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [ 4 16] are constant.\n",
    "  UserWarning)\n",
    "(78999, 13)\n",
    "Get the features that were selected: Probe\n",
    "true=selector.get_support()\n",
    "newcolindex_Probe=[i for i, x in enumerate(true) if x]\n",
    "newcolname_Probe=list( colNames[i] for i in newcolindex_Probe )\n",
    "newcolname_Probe\n",
    "['logged_in',\n",
    " 'rerror_rate',\n",
    " 'srv_rerror_rate',\n",
    " 'dst_host_srv_count',\n",
    " 'dst_host_diff_srv_rate',\n",
    " 'dst_host_same_src_port_rate',\n",
    " 'dst_host_srv_diff_host_rate',\n",
    " 'dst_host_rerror_rate',\n",
    " 'dst_host_srv_rerror_rate',\n",
    " 'Protocol_type_icmp',\n",
    " 'service_eco_i',\n",
    " 'service_private',\n",
    " 'flag_SF']\n",
    "X_newR2L = selector.fit_transform(X_R2L,Y_R2L)\n",
    "X_newR2L.shape\n",
    "C:\\Users\\Cynthia\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [  4  16  43  44  46  47  48  49  50  51  54  57  58  62  63  64  66  67\n",
    "  68  70  71  72  73  74  76  77  78  79  80  81  82  83  86  87  89  92\n",
    "  93  96  98  99 100 107 108 109 110 114] are constant.\n",
    "  UserWarning)\n",
    "(68338, 13)\n",
    "Get the features that were selected: R2L\n",
    "true=selector.get_support()\n",
    "newcolindex_R2L=[i for i, x in enumerate(true) if x]\n",
    "newcolname_R2L=list( colNames[i] for i in newcolindex_R2L)\n",
    "newcolname_R2L\n",
    "['src_bytes',\n",
    " 'dst_bytes',\n",
    " 'hot',\n",
    " 'num_failed_logins',\n",
    " 'is_guest_login',\n",
    " 'dst_host_srv_count',\n",
    " 'dst_host_same_src_port_rate',\n",
    " 'dst_host_srv_diff_host_rate',\n",
    " 'service_ftp',\n",
    " 'service_ftp_data',\n",
    " 'service_http',\n",
    " 'service_imap4',\n",
    " 'flag_RSTO']\n",
    "X_newU2R = selector.fit_transform(X_U2R,Y_U2R)\n",
    "X_newU2R.shape\n",
    "C:\\Users\\Cynthia\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [  4  16  43  44  46  47  48  49  50  51  54  57  58  62  63  64  66  67\n",
    "  68  70  71  72  73  74  75  76  77  78  79  80  81  82  83  86  87  89\n",
    "  92  93  96  98  99 100 107 108 109 110 114] are constant.\n",
    "  UserWarning)\n",
    "(67395, 13)\n",
    "Get the features that were selected: U2R\n",
    "true=selector.get_support()\n",
    "newcolindex_U2R=[i for i, x in enumerate(true) if x]\n",
    "newcolname_U2R=list( colNames[i] for i in newcolindex_U2R)\n",
    "newcolname_U2R\n",
    "['urgent',\n",
    " 'hot',\n",
    " 'root_shell',\n",
    " 'num_file_creations',\n",
    " 'num_shells',\n",
    " 'srv_diff_host_rate',\n",
    " 'dst_host_count',\n",
    " 'dst_host_srv_count',\n",
    " 'dst_host_same_src_port_rate',\n",
    " 'dst_host_srv_diff_host_rate',\n",
    " 'service_ftp_data',\n",
    " 'service_http',\n",
    " 'service_telnet']\n",
    "Summary of features selected by Univariate Feature Selection\n",
    "print('Features selected for DoS:',newcolname_DoS)\n",
    "print()\n",
    "print('Features selected for Probe:',newcolname_Probe)\n",
    "print()\n",
    "print('Features selected for R2L:',newcolname_R2L)\n",
    "print()\n",
    "print('Features selected for U2R:',newcolname_U2R)\n",
    "Features selected for DoS: ['logged_in', 'count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'service_http', 'flag_S0', 'flag_SF']\n",
    "\n",
    "Features selected for Probe: ['logged_in', 'rerror_rate', 'srv_rerror_rate', 'dst_host_srv_count', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'Protocol_type_icmp', 'service_eco_i', 'service_private', 'flag_SF']\n",
    "\n",
    "Features selected for R2L: ['src_bytes', 'dst_bytes', 'hot', 'num_failed_logins', 'is_guest_login', 'dst_host_srv_count', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'service_ftp', 'service_ftp_data', 'service_http', 'service_imap4', 'flag_RSTO']\n",
    "\n",
    "Features selected for U2R: ['urgent', 'hot', 'root_shell', 'num_file_creations', 'num_shells', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'service_ftp_data', 'service_http', 'service_telnet']\n",
    "The authors state that \"After obtaining the adequate number of features during the univariate selection process, a recursive feature elimination (RFE) was operated with the number of features passed as parameter to identify the features selected\". This either implies that RFE is only used for obtaining the features previously selected but also obtaining the rank. This use of RFE is however very redundant as the features selected can be obtained in another way (Done in this project). One can also not say that the features were selected by RFE, as it was not used for this. The quote could however also imply that only the number 13 from univariate feature selection was used. RFE is then used for feature selection trying to find the best 13 features. With this use of RFE one can actually say that it was used for feature selection. However the authors obtained different numbers of features for every attack category, 12 for DoS, 15 for Probe, 13 for R2L and 11 for U2R. This concludes that it is not clear what mechanism is used for feature selection.\n",
    "To procede with the data mining, the second option is considered as this uses RFE. From now on the number of features for every attack category is 13.\n",
    "2. Recursive Feature Elimination for feature ranking (Option 1: get importance from previous selected)\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Create a decision tree classifier. By convention, clf means 'classifier'\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "#rank all features, i.e continue the elimination until the last one\n",
    "rfe = RFE(clf, n_features_to_select=1)\n",
    "rfe.fit(X_newDoS, Y_DoS)\n",
    "print (\"DoS Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_DoS)))\n",
    "DoS Features sorted by their rank:\n",
    "[(1, 'same_srv_rate'), (2, 'count'), (3, 'flag_SF'), (4, 'dst_host_serror_rate'), (5, 'dst_host_same_srv_rate'), (6, 'dst_host_srv_count'), (7, 'dst_host_count'), (8, 'logged_in'), (9, 'serror_rate'), (10, 'dst_host_srv_serror_rate'), (11, 'srv_serror_rate'), (12, 'service_http'), (13, 'flag_S0')]\n",
    "rfe.fit(X_newProbe, Y_Probe)\n",
    "print (\"Probe Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_Probe)))\n",
    "Probe Features sorted by their rank:\n",
    "[(1, 'dst_host_same_src_port_rate'), (2, 'dst_host_srv_count'), (3, 'dst_host_rerror_rate'), (4, 'service_private'), (5, 'logged_in'), (6, 'dst_host_diff_srv_rate'), (7, 'dst_host_srv_diff_host_rate'), (8, 'flag_SF'), (9, 'service_eco_i'), (10, 'rerror_rate'), (11, 'Protocol_type_icmp'), (12, 'dst_host_srv_rerror_rate'), (13, 'srv_rerror_rate')]\n",
    "rfe.fit(X_newR2L, Y_R2L)\n",
    " \n",
    "print (\"R2L Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_R2L)))\n",
    "R2L Features sorted by their rank:\n",
    "[(1, 'src_bytes'), (2, 'dst_bytes'), (3, 'hot'), (4, 'dst_host_srv_diff_host_rate'), (5, 'service_ftp_data'), (6, 'dst_host_same_src_port_rate'), (7, 'dst_host_srv_count'), (8, 'num_failed_logins'), (9, 'service_imap4'), (10, 'is_guest_login'), (11, 'service_ftp'), (12, 'flag_RSTO'), (13, 'service_http')]\n",
    "rfe.fit(X_newU2R, Y_U2R)\n",
    " \n",
    "print (\"U2R Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_U2R)))\n",
    "U2R Features sorted by their rank:\n",
    "[(1, 'hot'), (2, 'dst_host_srv_count'), (3, 'dst_host_count'), (4, 'root_shell'), (5, 'num_shells'), (6, 'service_ftp_data'), (7, 'dst_host_srv_diff_host_rate'), (8, 'num_file_creations'), (9, 'dst_host_same_src_port_rate'), (10, 'service_telnet'), (11, 'srv_diff_host_rate'), (12, 'service_http'), (13, 'urgent')]\n",
    "2. Recursive Feature Elimination, select 13 features each of 122 (Option 2: get 13 best features from 122 from RFE)\n",
    "from sklearn.feature_selection import RFE\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "rfe = RFE(estimator=clf, n_features_to_select=13, step=1)\n",
    "rfe.fit(X_DoS, Y_DoS)\n",
    "X_rfeDoS=rfe.transform(X_DoS)\n",
    "true=rfe.support_\n",
    "rfecolindex_DoS=[i for i, x in enumerate(true) if x]\n",
    "rfecolname_DoS=list(colNames[i] for i in rfecolindex_DoS)\n",
    "rfe.fit(X_Probe, Y_Probe)\n",
    "X_rfeProbe=rfe.transform(X_Probe)\n",
    "true=rfe.support_\n",
    "rfecolindex_Probe=[i for i, x in enumerate(true) if x]\n",
    "rfecolname_Probe=list(colNames[i] for i in rfecolindex_Probe)\n",
    "rfe.fit(X_R2L, Y_R2L)\n",
    "X_rfeR2L=rfe.transform(X_R2L)\n",
    "true=rfe.support_\n",
    "rfecolindex_R2L=[i for i, x in enumerate(true) if x]\n",
    "rfecolname_R2L=list(colNames[i] for i in rfecolindex_R2L)\n",
    "rfe.fit(X_U2R, Y_U2R)\n",
    "X_rfeU2R=rfe.transform(X_U2R)\n",
    "true=rfe.support_\n",
    "rfecolindex_U2R=[i for i, x in enumerate(true) if x]\n",
    "rfecolname_U2R=list(colNames[i] for i in rfecolindex_U2R)\n",
    "Summary of features selected by RFE\n",
    "print('Features selected for DoS:',rfecolname_DoS)\n",
    "print()\n",
    "print('Features selected for Probe:',rfecolname_Probe)\n",
    "print()\n",
    "print('Features selected for R2L:',rfecolname_R2L)\n",
    "print()\n",
    "print('Features selected for U2R:',rfecolname_U2R)\n",
    "Features selected for DoS: ['src_bytes', 'dst_bytes', 'wrong_fragment', 'num_compromised', 'same_srv_rate', 'diff_srv_rate', 'dst_host_count', 'dst_host_same_srv_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'service_ecr_i', 'flag_RSTR', 'flag_S0']\n",
    "\n",
    "Features selected for Probe: ['src_bytes', 'dst_bytes', 'rerror_rate', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_rerror_rate', 'service_finger', 'service_ftp_data', 'service_http', 'service_private', 'service_smtp', 'service_telnet']\n",
    "\n",
    "Features selected for R2L: ['duration', 'src_bytes', 'dst_bytes', 'hot', 'num_failed_logins', 'num_access_files', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'service_ftp_data', 'service_imap4']\n",
    "\n",
    "Features selected for U2R: ['duration', 'src_bytes', 'dst_bytes', 'hot', 'root_shell', 'num_file_creations', 'num_shells', 'srv_count', 'dst_host_count', 'dst_host_same_srv_rate', 'dst_host_srv_diff_host_rate', 'service_ftp_data', 'service_other']\n",
    "print(X_rfeDoS.shape)\n",
    "print(X_rfeProbe.shape)\n",
    "print(X_rfeR2L.shape)\n",
    "print(X_rfeU2R.shape)\n",
    "(113270, 13)\n",
    "(78999, 13)\n",
    "(68338, 13)\n",
    "(67395, 13)\n",
    "Step 4: Build the model:\n",
    "Classifier is trained for all features and for reduced features, for later comparison.\n",
    "The classifier model itself is stored in the clf variable.\n",
    "# all features\n",
    "clf_DoS=DecisionTreeClassifier(random_state=0)\n",
    "clf_Probe=DecisionTreeClassifier(random_state=0)\n",
    "clf_R2L=DecisionTreeClassifier(random_state=0)\n",
    "clf_U2R=DecisionTreeClassifier(random_state=0)\n",
    "clf_DoS.fit(X_DoS, Y_DoS)\n",
    "clf_Probe.fit(X_Probe, Y_Probe)\n",
    "clf_R2L.fit(X_R2L, Y_R2L)\n",
    "clf_U2R.fit(X_U2R, Y_U2R)\n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=0, splitter='best')\n",
    "# selected features\n",
    "clf_rfeDoS=DecisionTreeClassifier(random_state=0)\n",
    "clf_rfeProbe=DecisionTreeClassifier(random_state=0)\n",
    "clf_rfeR2L=DecisionTreeClassifier(random_state=0)\n",
    "clf_rfeU2R=DecisionTreeClassifier(random_state=0)\n",
    "clf_rfeDoS.fit(X_rfeDoS, Y_DoS)\n",
    "clf_rfeProbe.fit(X_rfeProbe, Y_Probe)\n",
    "clf_rfeR2L.fit(X_rfeR2L, Y_R2L)\n",
    "clf_rfeU2R.fit(X_rfeU2R, Y_U2R)\n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=0, splitter='best')\n",
    "Step 5: Prediction & Evaluation (validation):\n",
    "Using all Features for each category\n",
    "Confusion Matrices\n",
    "DoS\n",
    "# Apply the classifier we trained to the test data (which it has never seen before)\n",
    "clf_DoS.predict(X_DoS_test)\n",
    "array([1, 1, 0, ..., 0, 0, 0], dtype=int64)\n",
    "# View the predicted probabilities of the first 10 observations\n",
    "clf_DoS.predict_proba(X_DoS_test)[0:10]\n",
    "array([[ 0.,  1.],\n",
    "       [ 0.,  1.],\n",
    "       [ 1.,  0.],\n",
    "       [ 1.,  0.],\n",
    "       [ 1.,  0.],\n",
    "       [ 1.,  0.],\n",
    "       [ 1.,  0.],\n",
    "       [ 0.,  1.],\n",
    "       [ 1.,  0.],\n",
    "       [ 1.,  0.]])\n",
    "Y_DoS_pred=clf_DoS.predict(X_DoS_test)\n",
    "# Create confusion matrix\n",
    "pd.crosstab(Y_DoS_test, Y_DoS_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])\n",
    "Predicted attacks\t0\t1\n",
    "Actual attacks\t\t\n",
    "0\t9499\t212\n",
    "1\t2830\t4630\n",
    "Probe\n",
    "Y_Probe_pred=clf_Probe.predict(X_Probe_test)\n",
    "# Create confusion matrix\n",
    "pd.crosstab(Y_Probe_test, Y_Probe_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])\n",
    "Predicted attacks\t0\t2\n",
    "Actual attacks\t\t\n",
    "0\t2337\t7374\n",
    "2\t212\t2209\n",
    "R2L\n",
    "Y_R2L_pred=clf_R2L.predict(X_R2L_test)\n",
    "# Create confusion matrix\n",
    "pd.crosstab(Y_R2L_test, Y_R2L_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])\n",
    "Predicted attacks\t0\t3\n",
    "Actual attacks\t\t\n",
    "0\t9707\t4\n",
    "3\t2573\t312\n",
    "U2R\n",
    "Y_U2R_pred=clf_U2R.predict(X_U2R_test)\n",
    "# Create confusion matrix\n",
    "pd.crosstab(Y_U2R_test, Y_U2R_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])\n",
    "Predicted attacks\t0\t4\n",
    "Actual attacks\t\t\n",
    "0\t9703\t8\n",
    "4\t60\t7\n",
    "Cross Validation: Accuracy, Precision, Recall, F-measure\n",
    "DoS\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "accuracy = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='precision')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='recall')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='f1')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))\n",
    "Accuracy: 0.99639 (+/- 0.00341)\n",
    "Precision: 0.99505 (+/- 0.00477)\n",
    "Recall: 0.99665 (+/- 0.00483)\n",
    "F-measure: 0.99585 (+/- 0.00392)\n",
    "Probe\n",
    "accuracy = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='precision_macro')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='recall_macro')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='f1_macro')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))\n",
    "Accuracy: 0.99571 (+/- 0.00328)\n",
    "Precision: 0.99391 (+/- 0.00684)\n",
    "Recall: 0.99267 (+/- 0.00405)\n",
    "F-measure: 0.99329 (+/- 0.00512)\n",
    "R2L\n",
    "accuracy = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='precision_macro')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='recall_macro')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='f1_macro')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))\n",
    "Accuracy: 0.97952 (+/- 0.00984)\n",
    "Precision: 0.97216 (+/- 0.01610)\n",
    "Recall: 0.96978 (+/- 0.01337)\n",
    "F-measure: 0.97093 (+/- 0.01388)\n",
    "U2R\n",
    "accuracy = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='precision_macro')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='recall_macro')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='f1_macro')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))\n",
    "Accuracy: 0.99663 (+/- 0.00259)\n",
    "Precision: 0.86481 (+/- 0.08952)\n",
    "Recall: 0.91672 (+/- 0.10661)\n",
    "F-measure: 0.88628 (+/- 0.07462)\n",
    "RFECV for illustration\n",
    "%matplotlib inline\n",
    "print(__doc__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "# The \"accuracy\" scoring is proportional to the number of correct\n",
    "# classifications\n",
    "rfecv_DoS = RFECV(estimator=clf_DoS, step=1, cv=10, scoring='accuracy')\n",
    "rfecv_DoS.fit(X_DoS_test, Y_DoS_test)\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.title('RFECV DoS')\n",
    "plt.plot(range(1, len(rfecv_DoS.grid_scores_) + 1), rfecv_DoS.grid_scores_)\n",
    "plt.show()\n",
    "Automatically created module for IPython interactive environment\n",
    "\n",
    "rfecv_Probe = RFECV(estimator=clf_Probe, step=1, cv=10, scoring='accuracy')\n",
    "rfecv_Probe.fit(X_Probe_test, Y_Probe_test)\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.title('RFECV Probe')\n",
    "plt.plot(range(1, len(rfecv_Probe.grid_scores_) + 1), rfecv_Probe.grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "rfecv_R2L = RFECV(estimator=clf_R2L, step=1, cv=10, scoring='accuracy')\n",
    "rfecv_R2L.fit(X_R2L_test, Y_R2L_test)\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.title('RFECV R2L')\n",
    "plt.plot(range(1, len(rfecv_R2L.grid_scores_) + 1), rfecv_R2L.grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "rfecv_U2R = RFECV(estimator=clf_U2R, step=1, cv=10, scoring='accuracy')\n",
    "rfecv_U2R.fit(X_U2R_test, Y_U2R_test)\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.title('RFECV U2R')\n",
    "plt.plot(range(1, len(rfecv_U2R.grid_scores_) + 1), rfecv_U2R.grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "Using 13 Features for each category\n",
    "Confusion Matrices\n",
    "DoS\n",
    "# reduce test dataset to 13 features, use only features described in rfecolname_DoS etc.\n",
    "X_DoS_test2=X_DoS_test[:,rfecolindex_DoS]\n",
    "X_Probe_test2=X_Probe_test[:,rfecolindex_Probe]\n",
    "X_R2L_test2=X_R2L_test[:,rfecolindex_R2L]\n",
    "X_U2R_test2=X_U2R_test[:,rfecolindex_U2R]\n",
    "X_U2R_test2.shape\n",
    "(9778, 13)\n",
    "Y_DoS_pred2=clf_rfeDoS.predict(X_DoS_test2)\n",
    "# Create confusion matrix\n",
    "pd.crosstab(Y_DoS_test, Y_DoS_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])\n",
    "Predicted attacks\t0\t1\n",
    "Actual attacks\t\t\n",
    "0\t9602\t109\n",
    "1\t2625\t4835\n",
    "Probe\n",
    "Y_Probe_pred2=clf_rfeProbe.predict(X_Probe_test2)\n",
    "# Create confusion matrix\n",
    "pd.crosstab(Y_Probe_test, Y_Probe_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])\n",
    "Predicted attacks\t0\t2\n",
    "Actual attacks\t\t\n",
    "0\t8709\t1002\n",
    "2\t944\t1477\n",
    "R2L\n",
    "Y_R2L_pred2=clf_rfeR2L.predict(X_R2L_test2)\n",
    "# Create confusion matrix\n",
    "pd.crosstab(Y_R2L_test, Y_R2L_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])\n",
    "Predicted attacks\t0\t3\n",
    "Actual attacks\t\t\n",
    "0\t9649\t62\n",
    "3\t2560\t325\n",
    "U2R\n",
    "Y_U2R_pred2=clf_rfeU2R.predict(X_U2R_test2)\n",
    "# Create confusion matrix\n",
    "pd.crosstab(Y_U2R_test, Y_U2R_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])\n",
    "Predicted attacks\t0\t4\n",
    "Actual attacks\t\t\n",
    "0\t9706\t5\n",
    "4\t52\t15\n",
    "Cross Validation: Accuracy, Precision, Recall, F-measure\n",
    "DoS\n",
    "accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='precision')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='recall')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='f1')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))\n",
    "Accuracy: 0.99732 (+/- 0.00251)\n",
    "Precision: 0.99679 (+/- 0.00464)\n",
    "Recall: 0.99705 (+/- 0.00356)\n",
    "F-measure: 0.99692 (+/- 0.00288)\n",
    "Probe\n",
    "accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='precision_macro')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='recall_macro')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='f1_macro')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))\n",
    "Accuracy: 0.99085 (+/- 0.00559)\n",
    "Precision: 0.98674 (+/- 0.01180)\n",
    "Recall: 0.98467 (+/- 0.01027)\n",
    "F-measure: 0.98565 (+/- 0.00872)\n",
    "R2L\n",
    "accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='precision_macro')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='recall_macro')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='f1_macro')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))\n",
    "Accuracy: 0.97451 (+/- 0.00906)\n",
    "Precision: 0.96683 (+/- 0.01316)\n",
    "Recall: 0.96069 (+/- 0.01547)\n",
    "F-measure: 0.96367 (+/- 0.01300)\n",
    "U2R\n",
    "accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "precision = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='precision_macro')\n",
    "print(\"Precision: %0.5f (+/- %0.5f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='recall_macro')\n",
    "print(\"Recall: %0.5f (+/- %0.5f)\" % (recall.mean(), recall.std() * 2))\n",
    "f = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='f1_macro')\n",
    "print(\"F-measure: %0.5f (+/- %0.5f)\" % (f.mean(), f.std() * 2))\n",
    "Accuracy: 0.99652 (+/- 0.00319)\n",
    "Precision: 0.87747 (+/- 0.15709)\n",
    "Recall: 0.89183 (+/- 0.17196)\n",
    "F-measure: 0.87497 (+/- 0.11358)\n",
    "Stratified CV => Stays the same\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=StratifiedKFold(10), scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99732 (+/- 0.00251)\n",
    "accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=StratifiedKFold(10), scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99085 (+/- 0.00559)\n",
    "accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=StratifiedKFold(10), scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.97451 (+/- 0.00906)\n",
    "accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=StratifiedKFold(10), scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99652 (+/- 0.00319)\n",
    "CV 2, 5, 10, 30, 50 fold\n",
    "DoS\n",
    "accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=2, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99662 (+/- 0.00116)\n",
    "accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=5, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99703 (+/- 0.00057)\n",
    "accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99732 (+/- 0.00251)\n",
    "accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=30, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99726 (+/- 0.00390)\n",
    "accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=50, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99703 (+/- 0.00599)\n",
    "Probe\n",
    "accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=2, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99060 (+/- 0.00165)\n",
    "accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=5, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99102 (+/- 0.00230)\n",
    "accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99085 (+/- 0.00559)\n",
    "accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=30, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99151 (+/- 0.00731)\n",
    "accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=50, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99110 (+/- 0.01073)\n",
    "R2L\n",
    "accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=2, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.97134 (+/- 0.00174)\n",
    "accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=5, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.97380 (+/- 0.00588)\n",
    "accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.97451 (+/- 0.00906)\n",
    "accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=30, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.97459 (+/- 0.01652)\n",
    "accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=50, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.97531 (+/- 0.01820)\n",
    "U2R\n",
    "accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=2, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99601 (+/- 0.00021)\n",
    "accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=5, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99714 (+/- 0.00153)\n",
    "accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99652 (+/- 0.00319)\n",
    "accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=30, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99683 (+/- 0.00643)\n",
    "accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=50, scoring='accuracy')\n",
    "print(\"Accuracy: %0.5f (+/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\n",
    "Accuracy: 0.99652 (+/- 0.00692)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
